import hashlib
from pathlib import Path
import logging

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

from config import VECTORSTORE_DIR, EMBEDDING_MODEL, CHUNKS_DIR
import json


logging.basicConfig(level=logging.INFO)


class VectorStoreService:
    def __init__(self):
        # Ù„Ø§ Ù†Ø­Ù…Ù‘Ù„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¹Ù†Ø¯ init
        self.embeddings = None
        self.model_name = EMBEDDING_MODEL
        self.loaded_vectorstores = {}
        logging.info("VectorStoreService initialized (embeddings not loaded yet)")

    # ======================================================
    # ØªØ­Ù…ÙŠÙ„ Embeddings Ø¹Ù†Ø¯ Ø£ÙˆÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù…
    # ======================================================
    def get_embeddings(self):
        if self.embeddings is None:
            try:
                logging.info(f"â³ Loading HuggingFace Embeddings: {self.model_name} ...")
                self.embeddings = HuggingFaceEmbeddings(model_name=self.model_name)
                logging.info("âœ… HuggingFace Embeddings loaded successfully")
            except Exception as e:
                logging.error(f"âŒ Failed to load embeddings: {e}")
                self.embeddings = None
        return self.embeddings

    # ======================================================
    # Ø§Ø³Ù… Ø¢Ù…Ù† Ù„Ù„ØªØ®Ø²ÙŠÙ†
    # ======================================================
    def _safe_name(self, name: str) -> str:
        return hashlib.md5(name.encode("utf-8")).hexdigest()

        # ======================================================
    # Ø­ÙØ¸ Chunks ÙÙŠ Ù…Ù„Ù JSON
    # ======================================================
    def save_chunks_to_file(self, chunks, name: str):
        output_file = CHUNKS_DIR / f"{name}_chunks.json"

        data = []
        for i, c in enumerate(chunks):
            data.append({
                "chunk_id": i + 1,
                "text": c.page_content.strip(),
                "metadata": {
                    "source": c.metadata.get("source"),
                    "page": c.metadata.get("page"),
                }
            })

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logging.info(f"ğŸ§© ØªÙ… Ø­ÙØ¸ {len(data)} Chunks ÙÙŠ: {output_file}")


    # ======================================================
    # ØªØ­Ù…ÙŠÙ„ Ø£Ùˆ Ø¥Ù†Ø´Ø§Ø¡ VectorStore
    # ======================================================
    def load_or_create(self, pdf_path: str, name: str):
        embeddings = self.get_embeddings()
        if embeddings is None:
            logging.error("âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ù†Ø´Ø§Ø¡ VectorStore Ø¨Ø¯ÙˆÙ† Embeddings")
            return None

        safe_name = self._safe_name(name)
        store_path = VECTORSTORE_DIR / safe_name

        # --------------------------------------------------
        # 1) ØªØ­Ù…ÙŠÙ„ VectorStore Ù…ÙˆØ¬ÙˆØ¯
        # --------------------------------------------------
        if store_path.exists() and (store_path / "index.faiss").exists():
            try:
                logging.info(f"ğŸ”„ ØªØ­Ù…ÙŠÙ„ VectorStore Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ø§Ù‹: {name}")
                vs = FAISS.load_local(
                    str(store_path),
                    embeddings,
                    allow_dangerous_deserialization=True
                )
                return vs
            except Exception as e:
                logging.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ VectorStore Ù…ÙˆØ¬ÙˆØ¯: {e}")
                logging.info("â™»ï¸ Ø³ÙŠØªÙ… Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ù†Ø´Ø§Ø¡ VectorStore...")

        # --------------------------------------------------
        # 2) Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù
        # --------------------------------------------------
        if not Path(pdf_path).exists():
            logging.error(f"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù: {pdf_path}")
            return None

        store_path.mkdir(parents=True, exist_ok=True)

        try:
            logging.info(f"ğŸ“„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù PDF: {pdf_path}")

            loader = PyPDFLoader(pdf_path)
            raw_docs = loader.load()

            # --------------------------------------------------
            # Ø­Ù…Ø§ÙŠØ© Ù…Ù† Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ© Ø£Ùˆ Ø§Ù„ØªØ§Ù„ÙØ©
            # --------------------------------------------------
            valid_docs = []
            for d in raw_docs:
                content = (d.page_content or "").strip()
                if len(content) > 20:
                    valid_docs.append(d)

            if not valid_docs:
                logging.error("âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ ØµØ§Ù„Ø­ Ø¯Ø§Ø®Ù„ PDF Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ")
                return None

            # --------------------------------------------------
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ
            # --------------------------------------------------
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=150,
                separators=[
                    "\nØ§Ù„Ù…Ø§Ø¯Ø©",
                    "\nâ€¢",
                    "\n\n",
                    ".\n",
                    "\n",
                    " "
                ]
            )

            chunks = splitter.split_documents(valid_docs)

            # --------------------------------------------------
            # Ø­Ù…Ø§ÙŠØ© Ø¥Ø¶Ø§ÙÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ‚Ø³ÙŠÙ…
            # --------------------------------------------------
            clean_chunks = []
            for c in chunks:
                text = (c.page_content or "").strip()
                if len(text) > 30:
                    clean_chunks.append(c)

            if not clean_chunks:
                logging.error("âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Chunks ØµØ§Ù„Ø­Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ‚Ø³ÙŠÙ…")
                return None

            logging.info(f"ğŸ—ï¸ Ø¥Ù†Ø´Ø§Ø¡ VectorStore Ù…Ù† {len(clean_chunks)} Ù‚Ø·Ø¹Ø© Ù†ØµÙŠØ©")

            vs = FAISS.from_documents(clean_chunks, embeddings)
            vs.save_local(str(store_path))

            # ğŸ§© Ø­ÙØ¸ Ø§Ù„Ù€ chunks ÙÙŠ Ù…Ù„Ù Ù…Ø³ØªÙ‚Ù„
            self.save_chunks_to_file(clean_chunks, name)

            logging.info(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ­ÙØ¸ VectorStore Ø¨Ù†Ø¬Ø§Ø­: {name}")
            return vs

        except Exception as e:
            logging.exception("âŒ ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¡ VectorStore")
            return None


# ======================================================
# Instance ÙˆØ§Ø­Ø¯ ÙÙ‚Ø· Ù„Ù„Ø®Ø¯Ù…Ø©
# ======================================================
vectorstore_service = VectorStoreService()